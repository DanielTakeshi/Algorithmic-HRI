\documentclass[letterpaper, 10pt, conference]{ieeeconf}
\usepackage{algorithm,tikz,pgfplots,balance,caption,multicol,lipsum,bbm,verbatim}
\usepackage{amsmath,balance,multicol,url}
\input{includes/preamble.tex}

\title{\LARGE \bf Human-Guided DQN: Using Expert Human Gameplay Data on Atari
2600 Games for Deep Reinforcement Learning}
\author{Daniel Seita \\
Computer Science Division\\
University of California, Berkeley\\
Email: seita@berkeley.edu
}

\begin{document}
\maketitle

\begin{abstract}
Deep Reinforcement Learning is arguably the hottest and most popular subfield of
Artificial Intelligence. In large part, this was popularized due to the success
of agents in learning how to play Atari games from scratch, given only the input
screen pixels and the game reward as input -- in other words, exact how a human
would learn how to play. While there has been substantial follow-up work on how
to improve the performance of agents in such games, there has been very little
work that incorporates human guidance in the process. In this paper, we report
our progress about an idea for using human expert gameplay on Atari 2600 games
to boost the performance of Deep Reinforcement Learning agents. Specifically, we
focus on the Deep Q-Network algorithm, and during the exploration stage for
Q-Learning, we explore how substituting the random exploration with human
actions impacts gameplay. We report on progress for two Atari 2600 games
(Breakout and Space Invaders) and show the potential for this idea to eventually
improve the performance of DQN agents.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{sec:introduction}

The now-popular technique of deep learning can be used for challenging tasks in
reinforcement learning, where the job of an AI agent is not to perform
``simple'' classification as in~\cite{AlexNet2012}, but to learn from
high-dimensional, correlated data with a scalar reward signal that is noisy and
may exhibit complicated, long-term rewards. For instance,~\cite{mnih-dqn-2015}
combined model-free reinforcement learning with deep learning techniques to
develop an AI agent capable of learning how to play several Atari 2600 games at
a level matching or exceeding human performance. The AI only learned from the
game frames and the score, just like how a human would learn. Similar techniques
rely on Monte Carlo Tree Search~\cite{nips-atari-2014}, including the
well-publicized AlphaGo AI agent~\cite{silver-alphago-2016}.

Nonetheless, despite the progress advanced by neural networks, many questions
still remain about how exactly neural networks learn, and it is still unclear if
this underlying ``process'' is at all similar to the way that humans would
learn. One way to explore this question would be to try and directly incorporate
learning from demonstrations to boosting the performance of agents.

In this report, a human expert plays games, Breakout and Space Invaders, and we
augment the learning process of neural network agents with human data to
accelerate training to get fast, high-quality policies. This step involves two
main steps. The first is to train a classifier to map from game frames to
actions based on human data. The second step is to incorporate the classifier
during the exploration phase of the DQN agent, when it is following an
$\epsilon$-greedy policy. Rather than have the ``$\epsilon$ cases'' correspond
to \emph{random} actions, the AI can use those cases to follow the \emph{human
action}.

We report on the results of our classifier and the AI agents. We show that
standard convolutional neural networks (CNNs) can often identify the correct
actions for humans to take, but that combining this inside a DQN agent does not
generally improve performance that much, though there are several obvious
steps to take for future work. Ultimately, we hope to better understand the
human learning and deep learning processes that enable the corresponding
agents to successfully play Atari games and hope to eventually boost the DQN
process with human data.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}\label{sec:related_work}
% This section will definitely be shorter than the CS 287 version, especially
% because we had an extra page for that one. Also, I think I should aim to have
% this section end at or before the first column on page 2.

The Deep Q-Network (DQN) algorithm trains an AI agent using a variant of
Q-learning~\cite{Sutton_1998}. In standard Q-Learning for solving a Markov
Decision Process, one has state-action values $Q(s,a)$ for state $s$ and action
$a$. This is the expected sum of discounted rewards for the agent starting at
state $s$, taking action $a$, and from then on, playing optimally according to
the action determined by the policy.  With Atari games, the states are
\emph{sequences} of game frames $x_1,x_2,\ldots,x_t$ encountered during game
play\footnote{Technically,~\cite{mnih-dqn-2015} reports that states are
sequences of game frames \emph{and} actions: $x_1,a_1,x_2,\ldots,a_t$. When
doing Q-Learning, however, their code only considers four consecutive frames and
does not take into account actions other than the current one under
consideration.}. The optimal action-value function $Q$ obeys the \emph{Bellman
equation} identity: 
\begin{equation}\label{eq:bellman}
Q(s,a) = \mathbb{E}_{s'}\left[r + \gamma \cdot \max_{a'} Q(s',a') \mid s,a \right].
\end{equation}

The process of Q-Learning (or more generally, reinforcement learning) is to
estimate the Q-values using the Bellman equation as an iterative update.

The states are extremely high dimensional; even with downsampling, one frame is
an $(84\times 84)$-dimensional input, and storing all $Q(s,a)$ values explicitly
in a table is impractical.  Therefore, the $Q(s,a)$ values are
\emph{approximated} by a neural network parameterized by its weights $\theta$,
and it is $\theta$ that the Q-Learning algorithm must learn.

In practice,~\cite{mnih-dqn-2015} uses a variant of online Q-Learning (with an
$\epsilon$-greedy policy for exploration) with two key ideas: experience replay
for breaking the correlation among data points and a separate target network
for generating the target terms in Equation~\ref{eq:bellman} to increase the
algorithm's stability. The DQN trained with this variant of Q-Learning was able
to excel at many Atari games, especially fast-paced games with simple rules such
as Breakout. It was, however, weak on games such as Montezuma's Revenge, which
requires substantial long-term strategy.

There has been a surge of follow-up work for training agents to play Atari
games.  In~\cite{nips-atari-2014}, they augment training using data collected
\emph{offline} through the use of Monte-Carlo tree search planning. The
``offline player,'' while substantially better than DQN, cannot play in real
time, but can be used to improve DQN's performance. The work
of~\cite{Schaul2016} introduces prioritized experience replay to
train DQN agents faster since the most important transitions (with respect to
temporal difference error) would be considered more frequently. It is also
possible to boostrap DQN~\cite{NIPS2016_6501} by borrowing techniques from
the statistical method of boostrapping.

Th work of~\cite{DBLP:conf/icml/WangSHHLF16} presents a different neural network
architecture specialized for reinforcement learning,
and~\cite{DBLP:conf/aaai/HasseltGS16} proposes Double-DQN, which mitigates
the problem of the ``max'' operator using the same values to both select and
evaluate an action (thus leading to overoptimistic value estimates). At the time
of publication, it was the highest-quality DQN available, though it has since
been surpassed by~\cite{DBLP:conf/icml/MnihBMGLHSK16}, which proposes
asynchronous variants of DQN algorithms and uses an asynchronous actor-critic
model to achieve the state of the art Atari results. These results were finally
surpassed by the current state of the art
in~\cite{DBLP:journals/corr/JaderbergMCSLSK16}.

% Daniel: I better double check of all of these.
While there has been much work concerning the technical aspects of DQN and its
variants, there has been very little work on incorporating human aspects
specifically to Atari games, the only major work of which is
from~\cite{DBLP:journals/corr/HosuR16}. Otherwise, however, this is a broader
category of Learning from Demonstrations, a category which has been receiving
more popularity including the seminal work of Maximum Entropy
IRL~\cite{Ziebart_2008_6055} and DAGGER~\cite{DBLP:journals/jmlr/RossGB11}.
There has been more recent work about adjusting humans and the loss
function~\cite{conf/nips/KimFPP13}, human supervision of robotic
grasping~\cite{DBLP:journals/corr/LaskeyCLMKJDG16,DBLP:dblp_conf/icra/LaskeySHMPDG16}
along with that of cooperation with humans~\cite{NIPS2016_6420}.

The aim of this work is to resolve the gap between DQN (and more generally, Deep
Reinforcement Learning) and Learning from Demonstrations by augmenting the DQN
algorithm with data on human gameplay. We also hope to better understand the
connection between human learning versus deep learning.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem Statement and Idea}\label{sec:idea}

Our chief goal is to improve DQN by inserting a classifier trained on human
actions as part of the $\epsilon$-greedy policy practiced by Q-Learning. In
addition, we also hope to show that classifiers can successfully predict human
actions.

\subsection{Algorithm Details}\label{ssec:algorithm}

\begin{figure}[t]
\centering
\includegraphics[width=0.45\textwidth]{figures/dqn_with_human_data_graph.png}
\caption{\footnotesize
The overall picture for our Human-Guided DQN algorithm with regards to
$\epsilon$ decay during Q-Learning. During the exploration stage, instead of
playing random actions with probability $\epsilon$, we perform the action chosen
from our trained classifier with probability $\epsilon-0.1$, up until 1 million
steps, upon which time our classifier is ignored. Note that, as described in
\textbf{TODO TODO}, we adjust the number of steps taken to investigate the
impact of a longer exploration period.
}
\label{fig:human-guided-dqn}
\end{figure}

To ensure sufficient exploration of the state space, both standard Q-Learning
and standard DQN follow $\epsilon$-greedy policies, where the action to take at
state $s$ is selected to be $a = \argmax_a Q(s,a)$ with probability $1-\epsilon$
and randomly selected otherwise. The code from~\cite{mnih-dqn-2015} initializes
at $\epsilon=1.0$ and linearly anneals it down to 0.1 after the first one
million steps, and then fixes it thereafter.

Our objective is to provide potentially better state exploration by utilizing
human data. Rather than choose an action with probability $\epsilon$, which will
be high in the beginning, why not choose the action that a human would take? One
hopeful outcome is that this will ``bias'' the state exploration space towards
``better'' areas, and then standard DQN would continue to build upon that
positive exploration to obtain higher-quality rewards. In particular, we hope to
see that this method provides improvement in the beginning of the exploration
stage relative to standard DQN.

Figure~\ref{fig:human-guided-dqn} presents a picture of the overall pipeline.
During the first million training steps where $\epsilon$ is linearly annealed,
when the agent selects a random action, we usually (but not always) choose
instead the action chosen by the classifier trained on human data. We leave a
fixed probability of $\epsilon=0.1$ to choose random actions, in part because
certain games have actions which appear extremely infrequently (e.g., FIRE in
Breakout during human gameplay occurs around five times per game) but will be
executed via these random actions. We call our method \emph{Human-Guided DQN}.

\subsection{Methodology and Implementation}\label{ssec:implementation}

There are three major steps for the experiments: human gameplay, developing a
classifier to map from game frames to actions, and then plugging it into DQN. 

\subsubsection{Human Gameplay} To enable human gameplay, we modify the Arcade
Learning Environment (ALE)~\cite{bellemare13arcade} to enable a human to play.
Each time step, we save the RGB game screen, the action taken, and the reward
received. The human player is the author of this paper, who is an expert in
Breakout and Space Invaders with roughly twenty hours and eight hours of prior
gampeplay experience for these respective games.\footnote{This is in contrast to
the methodology from~\cite{mnih-dqn-2015}, where human experts had only two
hours of training.} We ultimately collected human gameplay data based on six
hours of Breakout and five hours of Space Invaders. Due to the time-consuming
nature of this work, we leave analysis on other Atari games to future work.

\subsubsection{Developing a Classifier} With the data from the human gameplay,
we apply all the standard preprocessing steps performed in~\cite{mnih-dqn-2015},
such as frame skipping and taking four consecutive (but non-skipped) frames to
form a state. We then build a CNN using the same architecture as the Q-Network
from~\cite{mnih-dqn-2015}, which uses three convolutional layers followed by two
fully connected layers, and has the number of outputs equal to the number of
actions chosen. As mentioned in Section~\ref{ssec:algorithm}, however, we filter
the actions so that those which happen infrequently or a fixed amount of times
per game are not considered (instead, they are played via the random actions or
the standard Q-Network in DQN). For Deep Learning code, we use the Theano
library~\cite{2016arXiv160502688short}.  Our classifier's code and supporting
documents are
open-source.\footnote{\url{https://github.com/DanielTakeshi/Algorithmic-HRI}}

\subsubsection{The DQN Algorithm} Upon developing a classifier, we rigorously
tuned it (see Section~\ref{ssec:results_classifier}) to identify the strongest
hyperparameters. We then modified a popular open-source implementation of DQN to
load in the model weights into a new network (but with the same architecture)
and to enable it to use the classifier during the training process. Again, our
code is open-source on
GitHub.\footnote{\url{https://github.com/DanielTakeshi/deep_q_rl}}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results: Human Gameplay}\label{sec:results_p1}

\subsection{Classifier Performance}\label{ssec:results_classifier}

% Daniel: I think I want to hold off on this until I regenerate the Breakout
% data using the correct cropping method.
\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{Classifier Performance on Breakout}
\label{tab:breakout}
\centering
\begin{tabular}{c c c c | c c c c}
\hline
Reg.  & $\lambda$ & Train & Valid & Reg.  & $\lambda$ & Train & Valid \\
\hline
$L_1$ & 0.00005   &   &   & $L_2$ & 0.00005   &   &  \\
$L_1$ & 0.0001    &   &   & $L_2$ & 0.0001    &   &  \\
$L_1$ & 0.0005    &   &   & $L_2$ & 0.0005    &   &  \\
$L_1$ & 0.005     &   &   & $L_2$ & 0.001     &   &  \\
$L_1$ & 0.001     &   &   & $L_2$ & 0.005     &   &  \\
$L_1$ & 0.05      &   &   & $L_2$ & 0.05      &   &  \\
$L_1$ & 0.01      &   &   & $L_2$ & 0.01      &   &  \\
\hline
\end{tabular}
\end{table}

% Whew ... I think these numbers should be all correct.
\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{Classifier Performance on Space Invaders}
\label{tab:space_invaders}
\centering
\begin{tabular}{c c c c | c c c c}
\hline
Reg.  & $\lambda$ & Train & Valid & Reg.  & $\lambda$ & Train & Valid \\
\hline
$L_1$ & 0.00005   & \textbf{96.3}  & 67.5  & $L_2$ & 0.00005   & \textbf{97.8}  & 66.0 \\
$L_1$ & 0.0001    & 94.7  & 68.2  & $L_2$ & 0.0001    & \textbf{97.8}  & 66.9 \\
$L_1$ & 0.0005    & 76.5  & \textbf{74.5}  & $L_2$ & 0.0005    & 96.5  & 68.4 \\
$L_1$ & 0.005     & 65.9  & 65.8  & $L_2$ & 0.005     & 81.0  & \textbf{72.7} \\
$L_1$ & 0.001     & 74.4  & 73.4  & $L_2$ & 0.001     & 95.1  & 68.2 \\
$L_1$ & 0.05      & 28.5  & 29.0  & $L_2$ & 0.05      & 64.7  & 64.0 \\
$L_1$ & 0.01      & 28.5  & 29.0  & $L_2$ & 0.01      & 75.9  & \textbf{72.7} \\
\hline
\end{tabular}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{figures/bar_charts_actions.png}
\caption{\footnotesize
The distribution of actions for Breakout (left) and Space Invaders (right)
within our training data for classification. This is the distribution obtained
\emph{after} pre-processing steps to eliminate most NOOP actions. For Breakout,
we randomly select $x$ NOOP actions out of the full set, where $x$ is the
average of the LEFT and RIGHT action counts. For Space Invaders, we kept one
third of the NOOP actions. F-L and F-R represent ``FIRE LEFT'' and ``FIRE
RIGHT'', respectively.
}
\label{fig:action_distribution}
\end{figure}

After collecting the human gameplay, we formed a dataset $\mathcal{D}$
consisting of state-action pairs $\mathcal{D}=\{\phi_i, a_i\}_{i=1}^N$
encountered during human gameplay, where $\phi_i$ consists of four $84\times 84$
consecutive (non-skipped) grayscale images $\phi_i =
(s_{i-3},s_{i-2},s_{i-1},s_i)$ and $a_i$ is the action chosen (by the human
player) after observing game frame $s_i$.

During normal human gameplay, the distribution of actions is skewed, presenting
a challenge for training a classifier. In Breakout, the NOOP action tends to be
chosen far more often than LEFT or RIGHT, and the FIRE action is designed to
occur only five times a game. We therefore do not incorporate FIRE in our
Breakout classifier and subsample NOOP actions. For Space Invaders, the FIRE
actions (also including FIRE LEFT and FIRE RIGHT) occur more frequently, so we
included them in the classifier, but still only kept a third of the NOOP
actions.  Thus, Breakout results in a balanced three-way classification task,
while Space Invaders results in a six-way classification task, with a slightly
more skewed action distribution.  Figure~\ref{fig:action_distribution} shows the
action distributions.

We built a CNN following the architecture from~\cite{mnih-dqn-2015}. The number
of output layers is equal to the number of actions.  This allows all
$Q(\phi_i,a_j)$ values to be determined for all actions $a_j$ during one pass
with $\phi_i$ as input. We applied the softmax at the end. We split the data
into training, validation, and testing sets, and tuned weights via either $L_1$
or $L_2$ regularization. We settled on the regularization method and parameter
value $\lambda$ via validation set performance.\footnote{In this setting, it is
probably OK to tune on the test set, but we decided to stick to best practices
and tune on the validation set.  For both games, the best-performing settings on
the validation set also corresponded to the best performing testing sets.}

The Breakout and Space Invaders tuning results are shown in
Tables~\ref{tab:breakout} and~\ref{tab:space_invaders}, respectively (bold
indicates the best settings). With low $\lambda$, the training can get
arbitrarily high performance, but performs poorly on the validation set. For
both games, we use only the net which performed best on the validation set in
our Human-Guided DQN.




\subsection{Classifier Investigation}

\begin{figure*}[t]
\centering
\includegraphics[width=0.30\textwidth]{figures/empty.png}
\caption{\footnotesize
This will represent examples of game frames (i.e. sequence of 4 game frames) for
classifier, just like I did in the presentation. I'll want the full width (i.e.
two columns) for this, with good and bad examples from each game. More details
should be provided in a table with tuned values.
}
\label{fig:example_game_frames}
\end{figure*}

Now provide example images? Results are shown in
Figure~\ref{fig:example_game_frames}.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results: Modified DQN}\label{sec:results_p2}

\begin{figure*}[t]
\centering
\includegraphics[width=0.30\textwidth]{figures/empty.png}
\caption{\footnotesize
This will be another full-page figure. Here I'll hope to have the plots
comparing my results with DQN results, with both action-value and rewards. Use
moving averages. I may need a second one of these.
}
\label{fig:human_dqn_performance}
\end{figure*}

\begin{figure}[t]
\centering
\includegraphics[width=0.30\textwidth]{figures/empty.png}
\caption{\footnotesize
TODO hopefully this will be the SI with human but with longer exploration
periods.
}
\label{fig:sp_inv_longer_exploration}
\end{figure}

Figure~\ref{fig:human_dqn_performance}

Figure~\ref{fig:sp_inv_longer_exploration}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}\label{sec:conclusions}

In this work, we have made efforts to use Learning from Demonstration techniques
to boost the performance of DQN agents on the problem of playing Atari games. We
collected many hours of human expert gameplay data on Breakout and Space
Invaders, and provided extensive evidence to show that a convolutional
neural network can, to a large extent predict the action of the human expert
given a sequence of game frames. A DQN agent then used this classifier as part
of our human-guided DQN algorithm. While the DQN results do not improve using
our algorithm, we believe there is still potential for algorithmic improvements
in this work. In future work, we will first run the algorithm with a larger
number of exploration steps to better see the effects of the DQN agent.  Second,
we will try the concept of \emph{human experience replay} and combine experience
replay from the agent's exploration with that of a human. Third, our more
elaborate goal is to shift gears and work on training attention
models~\cite{NIPS2014_5542,icml2015_xuc15}, the idea being that for these games,
there are only a few important signals that matter for score (e.g., is the ball
near the paddle in Breakout \emph{and} moving downwards?), and this can be
trained into an attention model. We will explore these directions of work in the
coming months.

\bibliographystyle{abbrv}
\bibliography{Daniel_Seita_Report_AHRI}
\end{document}
